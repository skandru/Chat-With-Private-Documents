{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://ai.meta.com/blog/meta-llama-3/\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='https://ai.meta.com/blog/meta-llama-3/', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='![Meta](https://scontent-\\niad3-1.xx.fbcdn.net/v/t39.8562-6/252294889_575082167077436_6034106545912333281_n.svg/meta-\\nlogo-\\nprimary_standardsize.svg?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=OsB0GaOl0Z4Q7kNvgFTxSxt&_nc_ht=scontent-\\niad3-1.xx&oh=00_AfCWPQ5Lien0vI4i39DhF65wHdEJsy8pKdJ7Sfm5AGe93g&oe=663E4FF9)\\n\\n* Our approach\\n* Research\\n* [Meta AI](/meta-ai/)\\n* [Meta Llama](https://llama.meta.com/)\\n* [Blog](/blog/)\\n* [Try Meta AI](https://www.meta.ai/?utm_source=ai_meta_site&utm_medium=web&utm_content=AI_nav&utm_campaign=April_moment)\\n* [](/)\\n\\nLarge Language Model\\n\\nIntroducing Meta Llama 3: The most capable openly available LLM to date\\n\\nApril 18, 2024\\n\\n  \\n\\nTakeaways:\\n\\nRECOMMENDED READS\\n\\n  * [5 Steps to Getting Started with Llama 2](https://ai.meta.com/blog/5-steps-to-getting-started-with-llama-2/)\\n  * [The Llama Ecosystem: Past, Present, and Future](https://ai.meta.com/blog/llama-2-updates-connect-2023/)\\n  * [Introducing Code Llama, a state-of-the-art large language model for coding](https://ai.meta.com/blog/code-llama-large-language-model-coding/)\\n  * [Meta and Microsoft Introduce the Next Generation of Llama](https://ai.meta.com/blog/llama-2/)\\n\\n  \\n\\n  * Today, we’re introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model.\\n  * Llama 3 models will soon be available on AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake, and with support from hardware platforms offered by AMD, AWS, Dell, Intel, NVIDIA, and Qualcomm.\\n  * We’re dedicated to developing Llama 3 in a responsible way, and we’re offering various resources to help others use it responsibly as well. This includes introducing new trust and safety tools with Llama Guard 2, Code Shield, and CyberSec Eval 2.\\n  * In the coming months, we expect to introduce new capabilities, longer context windows, additional model sizes, and enhanced performance, and we’ll share the Llama 3 research paper.\\n  * Meta AI, built with Llama 3 technology, is now one of the world’s leading AI assistants that can boost your intelligence and lighten your load—helping you learn, get things done, create content, and connect to make the most out of every moment. You can try Meta AI [_here_](https://meta.ai/).\\n\\n  \\n\\nToday, we’re excited to share the first two models of the next generation of\\nLlama, Meta Llama 3, available for broad use. This release features pretrained\\nand instruction-fine-tuned language models with 8B and 70B parameters that can\\nsupport a broad range of use cases. This next generation of Llama demonstrates\\nstate-of-the-art performance on a wide range of industry benchmarks and offers\\nnew capabilities, including improved reasoning. We believe these are the best\\nopen source models of their class, period. In support of our longstanding open\\napproach, we’re putting Llama 3 in the hands of the community. We want to\\nkickstart the next wave of innovation in AI across the stack—from applications\\nto developer tools to evals to inference optimizations and more. We can’t wait\\nto see what you build and look forward to your feedback.\\n\\nOur goals for Llama 3\\n\\n  \\n\\nWith Llama 3, we set out to build the best open models that are on par with\\nthe best proprietary models available today. We wanted to address developer\\nfeedback to increase the overall helpfulness of Llama 3 and are doing so while\\ncontinuing to play a leading role on responsible use and deployment of LLMs.\\nWe are embracing the open source ethos of releasing early and often to enable\\nthe community to get access to these models while they are still in\\ndevelopment. The text-based models we are releasing today are the first in the\\nLlama 3 collection of models. Our goal in the near future is to make Llama 3\\nmultilingual and multimodal, have longer context, and continue to improve\\noverall performance across core LLM capabilities such as reasoning and coding.\\n\\nState-of-the-art performance\\n\\nOur new 8B and 70B parameter Llama 3 models are a major leap over Llama 2 and\\nestablish a new state-of-the-art for LLM models at those scales. Thanks to\\nimprovements in pretraining and post-training, our pretrained and instruction-\\nfine-tuned models are the best models existing today at the 8B and 70B\\nparameter scale. Improvements in our post-training procedures substantially\\nreduced false refusal rates, improved alignment, and increased diversity in\\nmodel responses. We also saw greatly improved capabilities like reasoning,\\ncode generation, and instruction following making Llama 3 more steerable.\\n\\n  \\n\\n![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.2365-6/438037375_405784438908376_6082258861354187544_n.png?_nc_cat=106&ccb=1-7&_nc_sid=e280be&_nc_ohc=t8bz_2VzTMIQ7kNvgGjpd6r&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfAbk8DW6YV6G7l72n19Qnu2jY4aIjIbsH6zHUrn0lwNUA&oe=6652B44A)\\n\\n*Please see [_evaluation details_](https://github.com/meta-llama/llama3/blob/main/eval_details.md) for setting and parameters with which these evaluations are calculated.\\n\\n  \\n\\nIn the development of Llama 3, we looked at model performance on standard\\nbenchmarks and also sought to optimize for performance for real-world\\nscenarios. To this end, we developed a new high-quality human evaluation set.\\nThis evaluation set contains 1,800 prompts that cover 12 key use cases: asking\\nfor advice, brainstorming, classification, closed question answering, coding,\\ncreative writing, extraction, inhabiting a character/persona, open question\\nanswering, reasoning, rewriting, and summarization. To prevent accidental\\noverfitting of our models on this evaluation set, even our own modeling teams\\ndo not have access to it. The chart below shows aggregated results of our\\nhuman evaluations across of these categories and prompts against Claude\\nSonnet, Mistral Medium, and GPT-3.5.\\n\\n  \\n\\n![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.2365-6/438998263_1368970367138244_7396600838045603809_n.png?_nc_cat=111&ccb=1-7&_nc_sid=e280be&_nc_ohc=O8e1uTklrugQ7kNvgEBNmuA&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfBAHNoTwdYd53VgwDXQl_riFDnIotZlhD7aWAPLhzlE5g&oe=6652998F)\\n\\nPreference rankings by human annotators based on this evaluation set highlight\\nthe strong performance of our 70B instruction-following model compared to\\ncompeting models of comparable size in real-world scenarios.\\n\\nOur pretrained model also establishes a new state-of-the-art for LLM models at\\nthose scales.\\n\\n![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.2365-6/439014085_432870519293677_8138616034495713484_n.png?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=69E8CVIwm84Q7kNvgGqPVq1&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfCDHcvpupZuQT9vfS6Lwyg0vFiqa2oxhh2m95z4QHE6tw&oe=6652BB74)\\n\\n*Please see [_evaluation details_](https://github.com/meta-llama/llama3/blob/main/eval_details.md) for setting and parameters with which these evaluations are calculated.\\n\\n  \\n\\nTo develop a great language model, we believe it’s important to innovate,\\nscale, and optimize for simplicity. We adopted this design philosophy\\nthroughout the Llama 3 project with a focus on four key ingredients: the model\\narchitecture, the pretraining data, scaling up pretraining, and instruction\\nfine-tuning.\\n\\nModel architecture\\n\\nIn line with our design philosophy, we opted for a relatively standard\\ndecoder-only transformer architecture in Llama 3. Compared to Llama 2, we made\\nseveral key improvements. Llama 3 uses a tokenizer with a vocabulary of 128K\\ntokens that encodes language much more efficiently, which leads to\\nsubstantially improved model performance. To improve the inference efficiency\\nof Llama 3 models, we’ve adopted grouped query attention (GQA) across both the\\n8B and 70B sizes. We trained the models on sequences of 8,192 tokens, using a\\nmask to ensure self-attention does not cross document boundaries.\\n\\nTraining data\\n\\nTo train the best language model, the curation of a large, high-quality\\ntraining dataset is paramount. In line with our design principles, we invested\\nheavily in pretraining data. Llama 3 is pretrained on over 15T tokens that\\nwere all collected from publicly available sources. Our training dataset is\\nseven times larger than that used for Llama 2, and it includes four times more\\ncode. To prepare for upcoming multilingual use cases, over 5% of the Llama 3\\npretraining dataset consists of high-quality non-English data that covers over\\n30 languages. However, we do not expect the same level of performance in these\\nlanguages as in English.\\n\\nTo ensure Llama 3 is trained on data of the highest quality, we developed a\\nseries of data-filtering pipelines. These pipelines include using heuristic\\nfilters, NSFW filters, semantic deduplication approaches, and text classifiers\\nto predict data quality. We found that previous generations of Llama are\\nsurprisingly good at identifying high-quality data, hence we used Llama 2 to\\ngenerate the training data for the text-quality classifiers that are powering\\nLlama 3.\\n\\nWe also performed extensive experiments to evaluate the best ways of mixing\\ndata from different sources in our final pretraining dataset. These\\nexperiments enabled us to select a data mix that ensures that Llama 3 performs\\nwell across use cases including trivia questions, STEM, coding, historical\\nknowledge, _etc._\\n\\nScaling up pretraining\\n\\nTo effectively leverage our pretraining data in Llama 3 models, we put\\nsubstantial effort into scaling up pretraining. Specifically, we have\\ndeveloped a series of detailed scaling laws for downstream benchmark\\nevaluations. These scaling laws enable us to select an optimal data mix and to\\nmake informed decisions on how to best use our training compute. Importantly,\\nscaling laws allow us to predict the performance of our largest models on key\\ntasks (for example, code generation as evaluated on the HumanEval\\nbenchmark—see above) before we actually train the models. This helps us ensure\\nstrong performance of our final models across a variety of use cases and\\ncapabilities.\\n\\nWe made several new observations on scaling behavior during the development of\\nLlama 3. For example, while the Chinchilla-optimal amount of training compute\\nfor an 8B parameter model corresponds to ~200B tokens, we found that model\\nperformance continues to improve even after the model is trained on two orders\\nof magnitude more data. Both our 8B and 70B parameter models continued to\\nimprove log-linearly after we trained them on up to 15T tokens. Larger models\\ncan match the performance of these smaller models with less training compute,\\nbut smaller models are generally preferred because they are much more\\nefficient during inference.\\n\\nTo train our largest Llama 3 models, we combined three types of\\nparallelization: data parallelization, model parallelization, and pipeline\\nparallelization. Our most efficient implementation achieves a compute\\nutilization of over 400 TFLOPS per GPU when trained on 16K GPUs\\nsimultaneously. We performed training runs on two custom-built [_24K GPU\\nclusters_](https://engineering.fb.com/2024/03/12/data-center-\\nengineering/building-metas-genai-infrastructure/). To maximize GPU uptime, we\\ndeveloped an advanced new training stack that automates error detection,\\nhandling, and maintenance. We also greatly improved our hardware reliability\\nand detection mechanisms for silent data corruption, and we developed new\\nscalable storage systems that reduce overheads of checkpointing and rollback.\\nThose improvements resulted in an overall effective training time of more than\\n95%. Combined, these improvements increased the efficiency of Llama 3 training\\nby ~three times compared to Llama 2.\\n\\nInstruction fine-tuning\\n\\nTo fully unlock the potential of our pretrained models in chat use cases, we\\ninnovated on our approach to instruction-tuning as well. Our approach to post-\\ntraining is a combination of supervised fine-tuning (SFT), rejection sampling,\\nproximal policy optimization (PPO), and direct preference optimization (DPO).\\nThe quality of the prompts that are used in SFT and the preference rankings\\nthat are used in PPO and DPO has an outsized influence on the performance of\\naligned models. Some of our biggest improvements in model quality came from\\ncarefully curating this data and performing multiple rounds of quality\\nassurance on annotations provided by human annotators.\\n\\nLearning from preference rankings via PPO and DPO also greatly improved the\\nperformance of Llama 3 on reasoning and coding tasks. We found that if you ask\\na model a reasoning question that it struggles to answer, the model will\\nsometimes produce the right reasoning trace: The model knows how to produce\\nthe right answer, but it does not know how to select it. Training on\\npreference rankings enables the model to learn how to select it.\\n\\nBuilding with Llama 3\\n\\nOur vision is to enable developers to customize Llama 3 to support relevant\\nuse cases and to make it easier to adopt best practices and improve the open\\necosystem. With this release, we’re providing new trust and safety tools\\nincluding updated [_components_](https://github.com/meta-llama/PurpleLlama)\\nwith both Llama Guard 2 and Cybersec Eval 2, and the introduction of Code\\nShield—an inference time guardrail for filtering insecure code produced by\\nLLMs.\\n\\nWe’ve also co-developed Llama 3 with\\n[_torchtune_](https://github.com/pytorch/torchtune), the new PyTorch-native\\nlibrary for easily authoring, fine-tuning, and experimenting with LLMs.\\ntorchtune provides memory efficient and hackable training recipes written\\nentirely in PyTorch. The library is integrated with popular platforms such as\\nHugging Face, Weights & Biases, and EleutherAI and even supports Executorch\\nfor enabling efficient inference to be run on a wide variety of mobile and\\nedge devices. For everything from prompt engineering to using Llama 3 with\\nLangChain we have a comprehensive [_getting started\\nguide_](https://llama.meta.com/get-started/) and takes you from downloading\\nLlama 3 all the way to deployment at scale within your generative AI\\napplication.\\n\\nA system-level approach to responsibility\\n\\nWe have designed Llama 3 models to be maximally helpful while ensuring an\\nindustry leading approach to responsibly deploying them. To achieve this, we\\nhave adopted [_a new, system-level approach_](https://ai.meta.com/blog/meta-\\nllama-3-meta-ai-responsibility/) to the responsible development and deployment\\nof Llama. We envision Llama models as part of a broader system that puts the\\ndeveloper in the driver’s seat. Llama models will serve as a foundational\\npiece of a system that developers design with their unique end goals in mind.\\n\\n![](https://scontent-\\niad3-1.xx.fbcdn.net/v/t39.2365-6/438922663_1135166371264105_805978695964769385_n.png?_nc_cat=107&ccb=1-7&_nc_sid=e280be&_nc_ohc=vbYMdR01iVsQ7kNvgFChOAJ&_nc_ht=scontent-\\niad3-1.xx&oh=00_AfC-6k6bfVyadzqEozNPl4f5O3WSFEJZqpDe5KR3AtVdgg&oe=6652A21E)\\n\\nInstruction fine-tuning also plays a major role in ensuring the safety of our\\nmodels. Our instruction-fine-tuned models have been red-teamed (tested) for\\nsafety through internal and external efforts. \\u200b\\u200bOur red teaming approach\\nleverages human experts and automation methods to generate adversarial prompts\\nthat try to elicit problematic responses. For instance, we apply comprehensive\\ntesting to assess risks of misuse related to Chemical, Biological, Cyber\\nSecurity, and other risk areas. All of these efforts are iterative and used to\\ninform safety fine-tuning of the models being released. You can read more\\nabout our efforts in the [_model card_](https://github.com/meta-\\nllama/llama3/blob/main/MODEL_CARD.md).\\n\\nLlama Guard models are meant to be a foundation for prompt and response safety\\nand can easily be fine-tuned to create a new taxonomy depending on application\\nneeds. As a starting point, the new Llama Guard 2 uses the recently\\n[_announced_](https://mlcommons.org/2024/04/mlc-aisafety-v0-5-poc/) MLCommons\\ntaxonomy, in an effort to support the emergence of industry standards in this\\nimportant area. Additionally, CyberSecEval 2 expands on its predecessor by\\nadding measures of an LLM’s propensity to allow for abuse of its code\\ninterpreter, offensive cybersecurity capabilities, and susceptibility to\\nprompt injection attacks (learn more in [_our technical\\npaper_](https://ai.meta.com/research/publications/cyberseceval-2-a-wide-\\nranging-cybersecurity-evaluation-suite-for-large-language-models/)). Finally,\\nwe’re introducing Code Shield which adds support for inference-time filtering\\nof insecure code produced by LLMs. This offers mitigation of risks around\\ninsecure code suggestions, code interpreter abuse prevention, and secure\\ncommand execution.\\n\\nWith the speed at which the generative AI space is moving, we believe an open\\napproach is an important way to bring the ecosystem together and mitigate\\nthese potential harms. As part of that, we’re updating our [_Responsible Use\\nGuide_](https://llama.meta.com/responsible-use-guide) (RUG) that provides a\\ncomprehensive guide to responsible development with LLMs. As we outlined in\\nthe RUG, we recommend that all inputs and outputs be checked and filtered in\\naccordance with content guidelines appropriate to the application.\\nAdditionally, many cloud service providers offer content moderation APIs and\\nother tools for responsible deployment, and we encourage developers to also\\nconsider using these options.\\n\\nDeploying Llama 3 at scale\\n\\nLlama 3 will soon be available on all major platforms including cloud\\nproviders, model API providers, and much more. Llama 3 will be\\n[_everywhere_](https://llama.meta.com/get-started/).\\n\\nOur benchmarks show the tokenizer offers improved token efficiency, yielding\\nup to 15% fewer tokens compared to Llama 2. Also, Group Query Attention (GQA)\\nnow has been added to Llama 3 8B as well. As a result, we observed that\\ndespite the model having 1B more parameters compared to Llama 2 7B, the\\nimproved tokenizer efficiency and GQA contribute to maintaining the inference\\nefficiency on par with Llama 2 7B.\\n\\nFor examples of how to leverage all of these capabilities, check out [_Llama\\nRecipes_](https://github.com/meta-llama/llama-recipes) which contains all of\\nour open source code that can be leveraged for everything from fine-tuning to\\ndeployment to model evaluation.\\n\\nWhat’s next for Llama 3?\\n\\nThe Llama 3 8B and 70B models mark the beginning of what we plan to release\\nfor Llama 3. And there’s a lot more to come.\\n\\nOur largest models are over 400B parameters and, while these models are still\\ntraining, our team is excited about how they’re trending. Over the coming\\nmonths, we’ll release multiple models with new capabilities including\\nmultimodality, the ability to converse in multiple languages, a much longer\\ncontext window, and stronger overall capabilities. We will also publish a\\ndetailed research paper once we are done training Llama 3.\\n\\nTo give you a sneak preview for where these models are today as they continue\\ntraining, we thought we could share some snapshots of how our largest LLM\\nmodel is trending. Please note that this data is based on an early checkpoint\\nof Llama 3 that is still training and these capabilities are not supported as\\npart of the models released today.\\n\\n![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.2365-6/439015366_1603174683862748_5008894608826037916_n.png?_nc_cat=105&ccb=1-7&_nc_sid=e280be&_nc_ohc=EOG24ZxlvM0Q7kNvgG6W87w&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfCO_AOCnqCjRlHmfzOmcKBRo97b9aa1ZhHzV0tiFPdlNg&oe=6652BD2A)\\n\\n*Please see [_evaluation details_](https://github.com/meta-llama/llama3/blob/main/eval_details.md) for setting and parameters with which these evaluations are calculated.\\n\\n  \\n\\nWe’re committed to the continued growth and development of an open AI\\necosystem for releasing our models responsibly. We have long believed that\\nopenness leads to better, safer products, faster innovation, and a healthier\\noverall market. This is good for Meta, and it is good for society. We’re\\ntaking a community-first approach with Llama 3, and starting today, these\\nmodels are available on the leading cloud, hosting, and hardware platforms\\nwith many more to come.\\n\\nTry Meta Llama 3 today\\n\\nWe’ve integrated our latest models into Meta AI, which we believe is the\\nworld’s leading AI assistant. It’s now built with Llama 3 technology and it’s\\navailable in more countries across our apps.\\n\\nYou can use Meta AI on Facebook, Instagram, WhatsApp, Messenger, and [_the\\nweb_](https://meta.ai/) to get things done, learn, create, and connect with\\nthe things that matter to you. You can read more about the Meta AI experience\\n[_here_](https://about.fb.com/news/2024/04/meta-ai-assistant-built-with-\\nllama-3/).\\n\\nVisit the[ _Llama 3 website_](https://llama.meta.com/llama3) to download the\\nmodels and reference the[ _Getting Started Guide_](https://llama.meta.com/get-\\nstarted/) for the latest list of all available platforms.\\n\\nYou’ll also soon be able to test multimodal Meta AI on our Ray-Ban Meta smart\\nglasses.\\n\\nAs always, we look forward to seeing all the amazing products and experiences\\nyou will build with Meta Llama 3.\\n\\n* * *\\n\\nShare:[](https://www.facebook.com/sharer/sharer.php?u=https://ai.meta.com/blog/meta-\\nllama-3/)[](https://www.twitter.com/share?url=https://ai.meta.com/blog/meta-\\nllama-3/)[](https://www.linkedin.com/sharing/share-\\noffsite?url=https://ai.meta.com/blog/meta-llama-3/)\\n\\n* * *\\n\\nOur latest updates delivered to your inbox\\n\\n[Subscribe](https://ai.facebook.com/subscribe/) to our newsletter to keep up\\nwith Meta AI news, events, research breakthroughs, and more.\\n\\nJoin us in the pursuit of what’s possible with AI.\\n\\n[See all open\\npositions](https://www.metacareers.com/jobs/?is_leadership=0&sub_teams%5B0%5D=Artificial+Intelligence&is_in_page=0&fbclid=IwAR0O8BF7opOj5gASJmwYVGalPPXTLu-6xrl9w00eC7Rarp2HQ9uEH8tERFw)\\n\\nRelated Posts\\n\\n![](https://scontent-\\niad3-1.xx.fbcdn.net/v/t39.2365-6/338318848_238475658638014_6444534044370711549_n.gif?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=i1sT3MB0-r4Q7kNvgEUUDsg&_nc_ht=scontent-\\niad3-1.xx&oh=00_AfAoQDm9-ti5VNqU_h4TxM9KAfa8F08wQAVTpnpuWolejQ&oe=6652C769)\\n\\nComputer Vision\\n\\nIntroducing Segment Anything: Working toward the first foundation model for\\nimage segmentation\\n\\nApril 5, 2023\\n\\n[Read post](https://ai.meta.com/blog/segment-anything-foundation-model-image-\\nsegmentation/)\\n\\nFEATURED\\n\\n![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.2365-6/284099254_760295688673506_1047420741523524710_n.jpg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=ovzXD1xjXtYQ7kNvgEWuBrF&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfCkN8Y7n_Sye8cRT8nRBkUHuk1R5abyBgIa4MMhw9wRQA&oe=66529BB6)\\n\\nResearch\\n\\nMultiRay: Optimizing efficiency for large-scale AI models\\n\\nNovember 18, 2022\\n\\n[Read post](https://ai.meta.com/blog/multiray-large-scale-AI-models/)\\n\\nFEATURED\\n\\n![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.2365-6/334793505_583125787173687_542838236294006040_n.png?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=EnfdMS9TfpYQ7kNvgHuCO0_&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfBLAmlLHxHBnrvaUcKghDDDXVv0jOmUM2t5hwMiJp-aQQ&oe=6652AF44)\\n\\nML Applications\\n\\nMuAViC: The first audio-video speech translation benchmark\\n\\nMarch 8, 2023\\n\\n[Read post](https://ai.meta.com/blog/muavic-audio-visual-speech-translation-\\nbenchmark/)\\n\\n[Our approach](/about) __ __\\n\\n[About AI at Meta](/about)\\n\\n[ Responsibility](/responsible-ai)\\n\\n[People](/results/?content_types%5B0%5D=person&sort_by=random)\\n\\n[Careers](https://www.metacareers.com/jobs/?is_leadership=0&sub_teams\\\\[0\\\\]=Artificial%20Intelligence&is_in_page=0)\\n\\n[Research](/research) __ __\\n\\n[Infrastructure](/infrastructure)\\n\\n[ Resources](/resources)\\n\\n[Demos](/resources/demos/)\\n\\nProduct experiences\\n\\n __ __\\n\\n[Meta AI](/meta-ai/)\\n\\n[ Latest news](/blog) __ __\\n\\n[Blog](/blog)\\n\\n[ Newsletter](/subscribe)\\n\\nFoundational models\\n\\n __ __\\n\\n[Meta Llama](https://llama.meta.com/)\\n\\n![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.2365-6/87524316_2677189655726266_6338721200264445952_n.svg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=MD3GjYrvdigQ7kNvgE8Hfl0&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfB-ihW9QH4tPswULTO6OWYhy0kDNevr7MdeW_NNcDbI9w&oe=6652B9B8)\\n\\n[![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=2GJf5wgKLT4Q7kNvgGjNrgh&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfD4641lUkPIG8XiUpwzB2Sf4EKpfOsWJla1n47h_WDonA&oe=663E4027)![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=2GJf5wgKLT4Q7kNvgGjNrgh&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfD4641lUkPIG8XiUpwzB2Sf4EKpfOsWJla1n47h_WDonA&oe=663E4027)](https://www.facebook.com/aiatmeta/)\\n\\n[![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=WA9Xxe0EUsYQ7kNvgFmU8ce&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfDGwKg6dRfoffeoau1TwqVPCue-h1IfdVcNUGn3GR_Cwg&oe=663E3862)![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=WA9Xxe0EUsYQ7kNvgFmU8ce&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfDGwKg6dRfoffeoau1TwqVPCue-h1IfdVcNUGn3GR_Cwg&oe=663E3862)](https://twitter.com/aiatmeta/)\\n\\n[![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=gIXobFinm1wQ7kNvgF916Yp&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfAdHhtUafPaY8zRKr13satxJemJF5_1ZbbvUvc87gp-\\ncA&oe=663E2BFB)![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=gIXobFinm1wQ7kNvgF916Yp&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfAdHhtUafPaY8zRKr13satxJemJF5_1ZbbvUvc87gp-\\ncA&oe=663E2BFB)](https://www.linkedin.com/showcase/aiatmeta)\\n\\n[![](https://scontent-\\niad3-1.xx.fbcdn.net/v/t39.8562-6/335648731_142576991793348_7786819189843639239_n.svg?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=7xzH-\\nPqbfuoQ7kNvgHVInKY&_nc_ht=scontent-\\niad3-1.xx&oh=00_AfBnsXL_MAKXkI7hW2f3KILvjJsInW4kQeZjJFJeBzzP-g&oe=663E456E)![](https://scontent-\\niad3-1.xx.fbcdn.net/v/t39.8562-6/335648731_142576991793348_7786819189843639239_n.svg?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=7xzH-\\nPqbfuoQ7kNvgHVInKY&_nc_ht=scontent-\\niad3-1.xx&oh=00_AfBnsXL_MAKXkI7hW2f3KILvjJsInW4kQeZjJFJeBzzP-g&oe=663E456E)](https://www.youtube.com/@aiatmeta)\\n\\nOur approach\\n\\n __ __\\n\\n[Our approach](/about)[ About AI at\\nMeta](/about)[Responsibility](/responsible-\\nai)[People](/results/?content_types%5B0%5D=person&sort_by=random)[Careers](https://www.metacareers.com/jobs/?is_leadership=0&sub_teams\\\\[0\\\\]=Artificial%20Intelligence&is_in_page=0)\\n\\nResearch\\n\\n __ __\\n\\n[Research](/research)[\\nInfrastructure](/infrastructure)[Resources](/resources)[Demos](/resources/demos/)\\n\\nProduct experiences\\n\\n __ __\\n\\n[Meta AI](/meta-ai/)\\n\\nLatest news\\n\\n __ __\\n\\n[Latest news](/blog)[ Blog](/blog)[Newsletter](/subscribe)\\n\\nFoundational models\\n\\n __ __\\n\\n[Meta Llama](https://llama.meta.com/)\\n\\n[![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=2GJf5wgKLT4Q7kNvgGjNrgh&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfD4641lUkPIG8XiUpwzB2Sf4EKpfOsWJla1n47h_WDonA&oe=663E4027)![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=2GJf5wgKLT4Q7kNvgGjNrgh&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfD4641lUkPIG8XiUpwzB2Sf4EKpfOsWJla1n47h_WDonA&oe=663E4027)](https://www.facebook.com/aiatmeta/)\\n\\n[![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=WA9Xxe0EUsYQ7kNvgFmU8ce&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfDGwKg6dRfoffeoau1TwqVPCue-h1IfdVcNUGn3GR_Cwg&oe=663E3862)![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=WA9Xxe0EUsYQ7kNvgFmU8ce&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfDGwKg6dRfoffeoau1TwqVPCue-h1IfdVcNUGn3GR_Cwg&oe=663E3862)](https://twitter.com/aiatmeta/)\\n\\n[![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=gIXobFinm1wQ7kNvgF916Yp&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfAdHhtUafPaY8zRKr13satxJemJF5_1ZbbvUvc87gp-\\ncA&oe=663E2BFB)![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=gIXobFinm1wQ7kNvgF916Yp&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfAdHhtUafPaY8zRKr13satxJemJF5_1ZbbvUvc87gp-\\ncA&oe=663E2BFB)](https://www.linkedin.com/showcase/aiatmeta)\\n\\n[![](https://scontent-\\niad3-1.xx.fbcdn.net/v/t39.8562-6/335648731_142576991793348_7786819189843639239_n.svg?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=7xzH-\\nPqbfuoQ7kNvgHVInKY&_nc_ht=scontent-\\niad3-1.xx&oh=00_AfBnsXL_MAKXkI7hW2f3KILvjJsInW4kQeZjJFJeBzzP-g&oe=663E456E)![](https://scontent-\\niad3-1.xx.fbcdn.net/v/t39.8562-6/335648731_142576991793348_7786819189843639239_n.svg?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=7xzH-\\nPqbfuoQ7kNvgHVInKY&_nc_ht=scontent-\\niad3-1.xx&oh=00_AfBnsXL_MAKXkI7hW2f3KILvjJsInW4kQeZjJFJeBzzP-g&oe=663E456E)](https://www.youtube.com/@aiatmeta)\\n\\n[ Privacy Policy](https://www.facebook.com/about/privacy/)\\n\\n[Terms](https://www.facebook.com/policies/)\\n\\n[Cookies](https://www.facebook.com/policies/cookies/)\\n\\nMeta © 2024\\n\\n[![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=2GJf5wgKLT4Q7kNvgGjNrgh&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfD4641lUkPIG8XiUpwzB2Sf4EKpfOsWJla1n47h_WDonA&oe=663E4027)![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/335682312_964107378293184_3093631164486164913_n.svg?_nc_cat=100&ccb=1-7&_nc_sid=e280be&_nc_ohc=2GJf5wgKLT4Q7kNvgGjNrgh&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfD4641lUkPIG8XiUpwzB2Sf4EKpfOsWJla1n47h_WDonA&oe=663E4027)](https://www.facebook.com/aiatmeta/)\\n\\n[![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=WA9Xxe0EUsYQ7kNvgFmU8ce&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfDGwKg6dRfoffeoau1TwqVPCue-h1IfdVcNUGn3GR_Cwg&oe=663E3862)![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/336009607_1870102080040414_6753977241281150924_n.svg?_nc_cat=103&ccb=1-7&_nc_sid=e280be&_nc_ohc=WA9Xxe0EUsYQ7kNvgFmU8ce&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfDGwKg6dRfoffeoau1TwqVPCue-h1IfdVcNUGn3GR_Cwg&oe=663E3862)](https://twitter.com/aiatmeta/)\\n\\n[![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=gIXobFinm1wQ7kNvgF916Yp&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfAdHhtUafPaY8zRKr13satxJemJF5_1ZbbvUvc87gp-\\ncA&oe=663E2BFB)![](https://scontent-\\niad3-2.xx.fbcdn.net/v/t39.8562-6/336289415_1541032296405649_2165099305308791297_n.svg?_nc_cat=109&ccb=1-7&_nc_sid=e280be&_nc_ohc=gIXobFinm1wQ7kNvgF916Yp&_nc_ht=scontent-\\niad3-2.xx&oh=00_AfAdHhtUafPaY8zRKr13satxJemJF5_1ZbbvUvc87gp-\\ncA&oe=663E2BFB)](https://www.linkedin.com/showcase/aiatmeta)\\n\\n[![](https://scontent-\\niad3-1.xx.fbcdn.net/v/t39.8562-6/335648731_142576991793348_7786819189843639239_n.svg?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=7xzH-\\nPqbfuoQ7kNvgHVInKY&_nc_ht=scontent-\\niad3-1.xx&oh=00_AfBnsXL_MAKXkI7hW2f3KILvjJsInW4kQeZjJFJeBzzP-g&oe=663E456E)![](https://scontent-\\niad3-1.xx.fbcdn.net/v/t39.8562-6/335648731_142576991793348_7786819189843639239_n.svg?_nc_cat=108&ccb=1-7&_nc_sid=e280be&_nc_ohc=7xzH-\\nPqbfuoQ7kNvgHVInKY&_nc_ht=scontent-\\niad3-1.xx&oh=00_AfBnsXL_MAKXkI7hW2f3KILvjJsInW4kQeZjJFJeBzzP-g&oe=663E456E)](https://www.youtube.com/@aiatmeta)\\n\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create llama3 model with llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex,  Settings\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"llama3\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")\n",
    "Settings.embed_model = ollama_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Introducing Meta Llama 3: The Next Generation of Large Language Models**\n",
      "\n",
      "Meta Llama 3 is the most capable openly available large language model (LLM) to date, designed to support a wide range of use cases. This state-of-the-art LLM offers improved reasoning capabilities and features pre-trained and instruction-fine-tuned models with 8B and 70B parameters.\n",
      "\n",
      "With **Meta Llama 3**, developers can customize the models to suit their specific needs and adopt best practices, ultimately driving innovation in AI across the stack. The model is designed to be maximally helpful while ensuring an industry-leading approach to responsible deployment.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* Pre-trained and instruction-fine-tuned language models with 8B and 70B parameters\n",
      "* Improved reasoning capabilities\n",
      "* Support for a broad range of use cases\n",
      "* Open source, available for community use and development\n",
      "\n",
      "**Try Meta Llama 3 Today!**\n",
      "\n",
      "Get started with the latest generation of large language models and unlock new possibilities in AI. With **Meta Llama 3**, you can create innovative applications, developer tools, evals, and inference optimizations that will revolutionize the way we interact with AI.\n",
      "\n",
      "**Learn More:**\n",
      "\n",
      "Visit the Meta Llama website to learn more about this groundbreaking technology and how it can benefit your organization or project. [Read More](https://llama.meta.com/)\n",
      "\n",
      "**Get Started Today!**\n",
      "\n",
      "Download **Meta Llama 3** and start building innovative applications, developer tools, evals, and inference optimizations that will change the game in AI.\n",
      "\n",
      "[Download Now](https://llama.meta.com/)\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Summarise What is Llama3 and make it SEO\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
